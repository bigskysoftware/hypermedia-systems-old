= Hypermedia In Action
:appendix-caption: A Brief, Incomplete, and Mostly Wrong History of Hypermedia
:sectnums:
:figure-caption: Figure {chapter}.
:listing-caption: Listing {chapter}.
:table-caption: Table {chapter}.
:sectnumoffset: 0
// line above:  :sectnumoffset: 5  (chapter# minus 1)
:leveloffset: 1
:sourcedir: ../code/src
:source-language:


Where did the idea of hypermedia come from?

While there were many precursors to the modern idea of hypertext and the more general hypermedia, many people point
to the article _"As We May Think"_ written by Vannevar Bush in The Atlantic as a starting point for looking at what
has become modern hypermedia.

In this article Bush described a device called a Memex, which, using a complex mechanical system of reels and microfilm,
along with an encoding system, would allow users to jump between related frames of content.  The Memex was never actually
implemented, but it was an inspiration for later work on the idea of hypermedia.

The terms "hypertext" and "hypermedia" were coined in 1963 by Ted Nelson, who would go on to work on the _Hypertext Editing
System_ and, later the _File Retrieval and Editing System (FRESS)_, a shockingly advanced system for its time.

While Nelson was working on his ideas, Douglas Engelbart was busy at work at the Stanford Research Institute, explicitly
attempting to make Vannevar Bush's Memex a reality.  In 1968, Englebart gave "The Mother of All Demos" in San Francisco,
California.

Englebart demonstrated an unbelievable amount of technology:

* Remote, collaborative text editing with his peers in Menlo Park
* Video and audio chat
* An integrated windowing system, with window resizing, etc.
* A recognizable hypertext, whereby clicking on underlined text navigated to new content

Despite a standing ovation after his talk, it was decades before these technologies became mainstream.

=== Implementation

In 1990, Tim Berners-Lee, working a CERN, published the first web site.  He had been working on the idea of hypertext
for a decade and was finally, out of desperation at the fact it was so hard for researchers to share their research,
found the right moment and institutional support to create the world wide web:

[quote, Tim Berners-Lee]
____
Creating the web was really an, because the situation without it was very difficult when I was working
at CERN later. Most of the technology involved in the web, like the hypertext, like the Internet, multifont text objects, had all
been designed already. I just had to put them together. It was a step of generalising, going to a higher level of abstraction,
thinking about all the documentation systems out there as being possibly part of a larger imaginary documentation system.
____

By 1994 the web had grown so massively that Berners-Lee founded the W3C, a working group of companies and researchers
tasked with improving the web.  All standards created by the W3C were royalty-free and could be adopted and implemented
by anyone, cementing the open, collaborative nature of the web.

In 2000, Roy Fielding published his seminal PhD Thesis: "Architectural Styles and the Design of Network-based Software
Architectures" at U.C. Irvine.  Fielding had been working on the open source Apache HTTP Server and his thesis
was a description of what he felt was a new and distinct networking architecture that had emerged in the early
web.  Fielding was responsible for the HTTP specification and, in the paper, defined the web's hypermedia
network model using the term _REpresentationalState Transfer (REST)_.

Fielding's paper became a touchstone for early web developers, giving them a language to discuss the new technical
medium they were building in.  Many developers today are familiar with the term REST only in terms of JSON APIs, but
it is important to remember that Fielding was describing _the web_, that is, the hypermedia, HTML-based system he
helped build with his thesis: JSON didn't even exist when he was writing!

We will take an in-depth look at Fielding's thesis in the next chapter.

=== Javascript & AJAX

In 1994 Netscape Navigator was released, quickly becoming the most popular browser on the web.  In 1995, LiveScript,
a scripting language that merged concepts from Scheme (a lisp variant) and Java (a hot language from Sun Mirosystems)
together, allowing users to create more dynamic behavior in the browser via client-side scripting.

It is worth noting that Fielding had explicitly allowed for client-side scripting in his paper on REST, in section 5.1.7,
entitled "Code-On-Demand"

[quote, Roy Fielding, https://www.ics.uci.edu/~fielding/pubs/dissertation/rest_arch_style.htm]
____
*Code-On-Demand*

The final addition to our constraint set for REST comes from the code-on-demand style of Section 3.5.3 (Figure 5-8).
REST allows client functionality to be extended by downloading and executing code in the form of applets or scripts. This
simplifies clients by reducing the number of features required to be pre-implemented. Allowing features to be
downloaded after deployment improves system extensibility. However, it also reduces visibility, and thus is
only an optional constraint within REST.
____

So scripting was a natural and sanctioned aspect of the new medium, the World Wide Web.

The new scripting language was renamed to JavaScript for marketing reasons and soon all major browsers had implemented
some form of the language.  In 1997, in an attempt to standardize the language across browsers, Netscape submitted
a proposal to ECMA International, leading to a specification known as ECMAScript.

In 1999, a new browser API was released by Microsoft: the `XMLHttpRequest` object.  This API allowed developers to
make HTTP requests directly from JavaScript, rather than using elements embedded in the DOM.  In 2005 the term
AJAX, short for "Asynchronous JavaScript and XML", was adopted to describe this new mechanism for building web
applications.  In 2006, the W3C released the first draft of a specification standardizing this API across all the
major browsers.

AJAX issued HTTP requests and, as the X in its name suggests, the response to these requests was often (althoug not
always) expected to be XML, a popular format in the early web.  Developers created XML APIs that could be used to
download contacts in XML format, for example, and that API could be used to dynamically populate web pages using
JavaScript.  The APIs, over time, came to be known as "Web Services".

=== Early Web Services (The XML Era)

The early Web Service development community quickly realized that many of these new XML APIs seemed different
than "regular" HTML-based web requests: the XML APIs often did not use hypermedia concepts, but rather were plain data APIs,
returning raw data without any additional context or information.  This fact was viewed with ambivalence: the web
had proven to be extremely flexible and vibrant, surely the core REST-ful concepts that it was built on should also be
part of this new approach as well!

==== The Richarson Maturity Model

In 2010, Martin Fowler proposed "The Richardson Maturity Model" as a measure of how "mature" a given web service was.

In this model, your web service API could "ranked" at one of the following levels:

1. Level 0: Plain Old XML
2. Level 1: Using Resources Properly
3. Level 2: Using HTTP Verbs Properly
4. Level 3: Using Hypermedia Controls Properly

===== Level 0: Plain Old XML

At this level, the XML API was simply exchanging plain XML with the client through arbitrary URLs.

Here is what a request might look like footnote:[NB: I will omit HTTP Headers for the sake of clarity in many of the examples]:

[#listing-1-3, reftext={chapter}.{counter:listing}]
.A POX HTTP Request
[source, http request]
----
GET /myApplicationServlet?handler=contact&ctx=WfVrDr0Y16yBSmjhXMNS1dOYZTsZ49dc&id=42&Operation=view HTTP/1.1
----

You can see that the path requested is essentially arbitrary.  Why `myApplicationServlet`?  If we wanted to update a
given contact what URL would we use?  It is hard to say given this URL schema.

Here is what a response might look like:

[#listing-1-4, reftext={chapter}.{counter:listing}]
.An XML Response
[source, xml]
----
<?xml version="1.0" encoding="UTF-8" ?>
<Contact>
  <FirstName>Jeff</FirstName>
  <LastName>Smith</LastName>
  <Phone>123-456-7890</Phone>
  <Email>jeff@example.com</Email>
</Contact>
----

If you are old enough, you will recognize this as an XML document, a file format that was popular around the time of
Martin Fowler's writing.  Note that the data here is "plain": we don't see any additional data beyond the names of
properties and their values.

This approach was disdainfully referred to as "The Swamp of POX", or Plain Old XML.  Fowler made the point that,
in adopting this technique for exchanging information with a remote system, engineers were abandoning the hypermedia
model entirely and were really using HTTP to implement their own Remote Procedure Call (RPC) mechanism.

===== Level 1: Resources

At this more mature level, URLs are organized into coherent *resources*, so, if, for example, you
wanted to retrieve the details for the contact with id `42`, you would issue a `GET` to
`/contacts/42`, where the path `contacts/42` represents a *resource* on the server that can be
retrieved:

[#listing-1-5, reftext={chapter}.{counter:listing}]
.A Resource Aware HTTP Request
[source, http request]
----
GET /contacts/42 HTTP/1.1
----

Here, contacts are being treated as a resource, and we are retrieving the contact with the id 42.  The URL organization
is coherent and treats particular paths as resources correctly.

The response to this request might look identical to the POX request in Level 0:

[#listing-1-4, reftext={chapter}.{counter:listing}]
.An XML Response
[source, xml]
----
<?xml version="1.0" encoding="UTF-8" ?>
<Contact>
  <FirstName>Jeff</FirstName>
  <LastName>Smith</LastName>
  <Phone>123-456-7890</Phone>
  <Email>jeff@example.com</Email>
</Contact>
----

This level of maturity (nor the next one) does not put any demands on the return content itself.

===== Level 2: HTTP Verbs

In another step up the maturity level, and API can support multiple HTTP Actions or Verbs for a
given resource: `GET` for retrieval, `POST` or `PUT` for updating and creating resources, and so on:

[#listing-1-5, reftext={chapter}.{counter:listing}]
.An HTTP Request Using Put To Update A Contact
[source, http request]
----
PUT /contacts/42 HTTP/1.1

first-name=Jeff&last-name=Smith&phone=123-456-7890&email=jeffsmith@example.com
----

Here we see a `PUT` being used to update a resource at the given URL.

The response to this request could be a redirect (to cause the client to issue a `GET` request), an XML representation
of the updated resource (if any) or an XML document indicating the result of the operation.  Again, at this level,
there is no significant constraint placed on the response content.

===== Level 3: Hypermedia Controls

The final and most mature level of an API, according to this model, was to adopt hypermedia
controls.  In all the examples above, the data being returned from the XML API was still a
simple XML representation of the resource.

At Level 3, the responses should include *hypermedia controls*, that is content indicating actions and relationships that
exist for that piece of data being represented.

[#listing-1-7, reftext={chapter}.{counter:listing}]
.An XML Response With Hypermedia Controls
[source, xml]
----
<?xml version="1.0" encoding="UTF-8" ?>
<Contact>
  <link rel="next" uri="/contacts/43"/>
  <FirstName>Jeff</FirstName>
  <LastName>Smith</LastName>
  <Phone>123-456-7890</Phone>
  <Email>jeff@example.com</Email>
</Contact>
----

Note the presence of a new tag in this XML, the `link` tag.  This link tag indicates that there is a _relationship_
between this resource and some other resource.  In this case, the relationship is that of "next", and the URL for the
next contact can be found at `/contacts/43` on the same server.

This is a _hypermedia control_: it embeds metadata about the resource that the client can interpret and use.

Fowler felt that there were two major benefits to this final level of hypermedia maturity:

* Servers can change their URI scheme without breaking clients
* The API was discoverable to developers working with the end points

both of which seem reasonably true.

===== Adoption

Despite these benefits, and even during the XML API era, when REST was a well known and hypermedia oriented concept
,it was rare for web services to reach the third level of Richardson maturity.  There were, at the time, heated arguments
around whether or not a particular API is REST-ful.  Over time, those arguments have, to a large extent, faded away.

Most APIs stopped at level 2 of the Richarson Maturity Model and simply published API documentation rather than embedding
hypermedia controls.  There were scattered examples of successful hypermedia controls in APIs, around paging and things
like that, but it never took off in the same way that HTML took off.

We will discuss web services or, as they are often called today Data APIs or just APIs, and why we think this might be
the case, in a later chapter.

=== Modern Web Services (The JSON Era)

While early Web Service APIs typically used XML, another format was rapidly gaining popularity among web developers: JSON.

JSON stands for "JavaScript Object Notation", a simple data format that is a subset of JavaScript itself.  The initial
specification was proposed by Douglas Crockford in the early 2000s and, in 2005, Yahoo began offering some of its
web services in JSON rather than XML.

If you were take the API above and render the response in JSON rather than XML, it would look something like this:

[#listing-1-7, reftext={chapter}.{counter:listing}]
.An XML Response
[source, json]
----
{
  "firstName" : "Jeff",
  "lastName" : "Smith",
  "phone" : "123-456-7890",
  "email" : "jeff@example.com"
}
----

This file format had many advantages.  In particular:

* It was terser
* It was easy to parse in JavaScript, which was becoming the primary consumer of Web Services

JSON ended up winning in a route, and took over the Web Service world entirely.  The vast majority of APIs being created
today are now JSON-based.

Something to notice about JSON is that, unlike XML, there is no obvious relationship to HTML.  XML still had a document
"flavor" to it, and the presence of link tags seemed like a reasonable, incremental step away from HTML.  The outline
of a hypermedia was still there.

JSON, on the other hand, is a plain data representation.  It becomes harder to see how hypermedia controls fit in with
this format.  It's possible to do, and some JSON APIs do include them, but, in moving to JSON as a response format, the
Web Service world, or, today, the JSON API world, took another step away from hypermedia.

.REST-ful JSON APIs?
****
A funny thing that happened along the way here was that the term REST, which was coined to described the HTML-based
web: it increasingly became associated with JSON APIs which were _not_, for the most part, REST-ful, at least in the original
sense of that term.  Today it is mostly JSON API engineers and you are unlikely to see or hear the term being discussed
among web developers.

This lead an exasperated Roy Fielding to say:

[quote, Roy Fielding, https://roy.gbiv.com/untangled/2008/rest-apis-must-be-hypertext-driven]
____
I am getting frustrated by the number of people calling any HTTP-based interface a REST API. Todayâ€™s example is the SocialSite REST API. That is RPC. It screams RPC. There is so much coupling on display that it should be given an X rating.

What needs to be done to make the REST architectural style clear on the notion that hypertext is a constraint? In other words, if the engine of application state (and hence the API) is not being driven by hypertext, then it cannot be RESTful and cannot be a REST API. Period. Is there some broken manual somewhere that needs to be fixed?
____

This book, in part, is an attempt to fix that "broken manual"!
****

=== The Emergence of Single-Page Applications (SPAs)

Early adopters of AJAX included Microsoft (Outlook Web Access) and Google (GMail, Google Maps).  By the early 2010s
AJAX was a hot technology, with developers clamoring for better tools to manage their increasingly complex JavaScript
code.

In 2010, Google released AngularJS, a framework for building what was becoming known as "Single Page Applications".
Single page applications did away with the traditional notion of HTML navigation via hyperlinks and replaced it with
dynamic content, managed by JavaScript and updated entirely via AJAX interactions, typically using JSON to communicate
with the server.

AngularJS was followed by React, from Facebook, in 2013.  React introduced the notion of reactive programming, where
a backing JavaScript model could be updated, and the DOM would automatically update to reflect the new state of the world.
This made management of JavaScript-based web applications much easier in some ways, but also pushed React-based web
applications further away from the original REST-ful model of the web in which *hypermedia* was intended to store (i.e. encode)
the state of the application.

As of this writing, React is king of the hill in Single Page Application frameworks, but there are many up and coming
challengers: Vue.js and Svelte.js are two examples.  Today, many web developers will automatically reach for these tools
for any web project that they work on and employers are clamoring for more React developers.

.Why Did Javascript & AJAX Win?
****
It is worth taking a step back at this point and ask: why did JavaScript and AJAX become so popular?  What need were they
satisfying?  The answer is that HTML and the hypermedia model of the web, for all the amazing aspects of them, felt
a little clunky when compared with "real" (that is, native) applications.  A user would click on a link and wait...
and, _eventually_, a whole new page of content would be downloaded and rendered onto the screen.  This often caused
visually-disturbing screen flickers, it reset the scroll position in the page, and so forth.  It could and can be
a jarring experience.

By using JavaScript and AJAX requests, the web could compete with native applications, smoothly updating content in
a web page without any flicker or other jarring visual issues.  Additionally, a richer UI event model was available to
Javascript: any event could drive a server request, not just clicks and submits.  This allowed web applications like
Google Maps to smoothly respond to scroll wheel events, dragging, etc. in a way that was simply impossible to
achieve in plain old HTML.
****

